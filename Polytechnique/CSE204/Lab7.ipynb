{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 204 Lab 7: Building a Neural Network From Scratch (Cont.)\n",
    "\n",
    "J.B. Scoggins\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jbscoggi/teaching/blob/master/Polytechnique/CSE204/Lab7.ipynb) \n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jbscoggi/teaching/master?filepath=Polytechnique%2FCSE204%2FLab7.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab you will learn to create a simple multilayer perceptron (MLP) neural network from scratch using only the Numpy package for matrix-vector operations.  In order to train the network on several datasets, you will need to implement the back-propagation algorithm with gradient descent.  Before getting started, let's review the notation we will use in this lab and recall the stochastic gradient descent algorithm.\n",
    "\n",
    "### Notation\n",
    "\n",
    "We will consider simple feed-forward networks that can be described by the following recursive relationship,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&z^l = a^{l-1} W^l + b^l,\\\\\n",
    "&a^l = \\sigma^l(z^l),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $a^l$ is the output (activation) of layer $l$ which is a nonlinear function $\\sigma^l$ of a linear transformation of the previous layer's output.  The linear transformation is performed using the weight matrix $W^l$ and bias vector $b^l$ associated with the layer $l$.  We will denote the last layer in the network with a capital $L$ superscript.  The recursion is stopped by setting $a^0 = x$, where $x$ is the input vector to our network.  Note that in the recursive expressions above, we implicitly assume that our input/output vectors are row vectors.  The reason for this will be evident later.\n",
    "\n",
    "Taking this notation into account, we see that a network with $L-1$ hidden layers is fully expressed by its $L$ weight matrices, bias vectors, and activation functions.  We can denote the set of trainable parameters in our network by $\\theta = \\{ W^1, \\dots, W^L, b^1, \\dots, b^L \\}$.\n",
    "\n",
    "In this lab, we are only concerned with supervised learning tasks.  Recall that in supervised learning, we have a dataset represented by a list of $(x, y)$ pairs where $x$ is the input to our model and $y$ is the desired output.\n",
    "\n",
    "- For regression problems where we want to fit a function $y = f(x)$, $x$ is the independent variable vector, and $y$ is the function value.\n",
    "- In classification problems, $x$ will correspond to a set of attributes and $y$ the corresponding label.\n",
    "\n",
    "The goal of supervised learning is to \"train\" our network by adjusting its parameters in order to minimize a cost function over the entire training set,\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\mathcal{L} = \\min_\\theta \\frac{1}{N} \\sum_{p=1}^N \\ell_p \n",
    "$$\n",
    "\n",
    "where $\\ell_p$ is just a short-hand notation for $\\ell_p = \\ell(a^L(x_p), y_p)$ and $\\ell(\\hat{y}, y)$ denotes the particular form of the loss function being considered.  In this lab, we will use 2 different loss functions:\n",
    "\n",
    "1. Quadratic Loss: $\\ell(\\hat{y}, y) = \\|\\hat{y} - y\\|^2 / 2$\n",
    "2. Cross-entropy Loss: $\\ell(\\hat{y}, y) = -\\sum_j[ y_j \\ln\\hat{y}_j + (1-y_j)\\ln(1-\\hat{y}_j)]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build an untrainable network\n",
    "\n",
    "Understanding (and implementing) the back-propagation algorithm can seem a little daunting at first.  Therefore, let's start by building out the functions we need just to create a network that cannot be trained.  First load the libraries.  _You should only have to do this once._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement the activation functions.\n",
    "\n",
    "1. The easiest activation we can implement is the identity function whic simply returns the input as itself.  Implement this below in the class template `Identity`.  The `prime` function should implement the derivative of the activation.\n",
    "2. The threshold activation function takes an input and returns 1 if the input is positive, otherwise 0.  Implement the `Threshold` class below. (hint: use the `np.where` function)\n",
    "3. Recall that the sigmoid function is given as $\\sigma(x) = 1 / (1 + \\exp(-x))$.  Derive the derivative of $\\sigma(x)$ and show that it can be represented as $\\sigma'(x) = \\sigma(x) [1 - \\sigma(x)]$\n",
    "4. Implement the `Sigmoid` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "class Identity:\n",
    "\n",
    "    @staticmethod\n",
    "    def __call__(x):\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(x):\n",
    "        return np.ones_like(x)\n",
    "\n",
    "class Threshold:\n",
    "\n",
    "    @staticmethod\n",
    "    def __call__(x):\n",
    "        return np.where(x > 0.0, 1.0, 0.0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def prime(x):\n",
    "        return np.zeros_like(x)\n",
    "\n",
    "    \n",
    "class Sigmoid:\n",
    "\n",
    "    @staticmethod\n",
    "    def __call__(x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(x):\n",
    "        sigmoid = Sigmoid()(x)\n",
    "        return sigmoid * (1.0 - sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plot the activastion functions and their derivative on the domain [-5, 5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-5, 5, 0.01)\n",
    "plt.plot(x, Sigmoid()(x))\n",
    "plt.plot(x, Sigmoid().prime(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this lab, we will build on the following python class called `Network` which will represent our neural net.  The `Network` class will keep track of the weights, biases, and activations needed to evaluate and train our model.  The following exercises will guide you through building up the class from the skeleton below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\"A simple implementation of a feed-forward artificial neural network.\n",
    "    \n",
    "    Work on this class throughout the entire lab, rerunning this cell after each update.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sizes, activations):\n",
    "        \"\"\"Construct a network given a list of the number of neurons in each layer and \n",
    "        a list of activations which will be applied to each layer.\n",
    "        \n",
    "        :param sizes: A list of integers representing the number of nodes in each layer, \n",
    "        including the input and output layers.\n",
    "        :param activations: A list of callable objects representing the activation functions.\n",
    "        Its size should be one less than sizes.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sizes = sizes\n",
    "        self.sigmas = activations\n",
    "        \n",
    "        self.biases  = [np.random.randn(1, n) for n in sizes[1:]]\n",
    "        self.weights = [\n",
    "            np.random.randn(m, n)/np.sqrt(m) for m, n in zip(sizes[:-1], sizes[1:])\n",
    "        ]\n",
    "    \n",
    "    def num_params(self):\n",
    "        \"\"\"Returns the total number of trainable parameters in this network.\"\"\"\n",
    "        return sum([m*n + n for m,n in zip(self.sizes[:-1], self.sizes[1:])])\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        \"\"\"Evaluates the network for the given input and returns the result.\n",
    "        \n",
    "        :param x: A numpy 2D-array where the columns represent input variables\n",
    "        and rows represent independent samples.\n",
    "        \"\"\"\n",
    "        a = x\n",
    "        for W, b, sigma in zip(self.weights, self.biases, self.sigmas):\n",
    "            a = sigma(np.matmul(a, W) + b)\n",
    "        return a\n",
    "        \n",
    "    \n",
    "    def backprop(self, x, y, cost):\n",
    "        \"\"\"Compute gradients of cost function w.r.t network parameters for a training point.\"\"\"\n",
    "        # Exercise 6.3\n",
    "        pass\n",
    "\n",
    "    def update_step(self, training_data, cost, learning_rate):\n",
    "        \"\"\"Compute the average parameter gradients over the whole training set and do a GD step.\n",
    "        \n",
    "        :param training_data: An (x, y) tuple containing the training data.\n",
    "        :param cost: A callable object with signature cost(y_pred, y) for computing the cost of a single\n",
    "        training example.\n",
    "        :param learning_rate: The learning rate to use in the GD step.\n",
    "        \"\"\"\n",
    "        # Exercise 6.4\n",
    "        pass\n",
    "    \n",
    "    def train(self, training_data, cost, learning_rate, epochs):\n",
    "        \"\"\"Trains this network on the training data\n",
    "        \n",
    "        :param training_data: An (x, y) tuple containing the training data.\n",
    "        :param cost: A callable object with signature cost(y_pred, y) for computing the cost of a single\n",
    "        training example.\n",
    "        :param learning_rate: The learning rate to use in the GD step.\n",
    "        :param epochs: The number of epochs to train with.\n",
    "        \"\"\"\n",
    "        for i in range(epochs):\n",
    "            self.update_step(training_data, cost, learning_rate)\n",
    "            \n",
    "            # Compute the loss\n",
    "            x, y = training_data\n",
    "            loss = cost(self.feed_forward(x), y) / x.shape[0]\n",
    "            print('epoch: ', i, ', loss: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: First draft of the Network class\n",
    "\n",
    "1. From the recursion formulas above, write down the shapes of the weight matrices and bias vectors, given the number of neurons in each layer.\n",
    "2. Implement the init `__init__` function in `Network` above.  Construct a list of weight matrices and bias vectors which are randomly initialized with normal distributions (see `np.random.randn`).\n",
    "2. Implement the `num_params` function in `Network` above.  Use your knowledge of the shapes of the weights and biases.\n",
    "3. Implement the `feed_forward` function in `Network` above. Use the recurance relations discussed in the Notation section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Test feed-forward Network initialization\n",
    "\n",
    "1. Create a network with 2 hidden layers of 4 nodes that takes 4 inputs and has 1 output. (Use any activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network( [4, 5, 5, 1], 3*[Sigmoid()] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute by hand the number of parameters this network should have. (61)\n",
    "3. Check that `num_params` provides the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(network.num_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Print the weights and biases of the network and confirm they are intialized correctly (shape and values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(network.weights, network.biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Build logic gates\n",
    "\n",
    "Below is a table of logical functions (logic gates).  Each function takes two values (A and B) representing True (1) or False (0) propositions and returns a True or False value.\n",
    "\n",
    "| A | B | AND | OR | XOR | NAND |\n",
    "|---|---|-----|----|-----|------|\n",
    "| 0 | 0 | 0   | 0  | 0   | 1    |\n",
    "| 0 | 1 | 0   | 1  | 1   | 1    |\n",
    "| 1 | 0 | 0   | 1  | 1   | 1    |\n",
    "| 1 | 1 | 1   | 1  | 0   | 0    |\n",
    "\n",
    "Interestingly, [it is possible to create any boolean function of any size through a combination of NAND gates](https://en.wikipedia.org/wiki/NAND_gate)!  Thus, if we can create a network which reproduces the logic behind a NAND-gate, it is possible to represent any logical function (and by extension any mathematical function) by combining such a network into ever more complex networks.  This is one version of a universal approximation theorem for ANNs.\n",
    "\n",
    "**Task:** build the AND, OR, and NAND logic gates above using the simple network below by modifying its weights and biases directly.  I have provided the `logic_gate` function to help test your networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logic_gate(network):\n",
    "    \"\"\"Helper function for testing our network as a logic gate.\"\"\"\n",
    "    return network.feed_forward(\n",
    "        np.asarray(\n",
    "            [[0, 0],\n",
    "            [0, 1],\n",
    "            [1, 0],\n",
    "            [1, 1]]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Create a simple \"logic gate\" network\n",
    "network = Network([2,1], [Threshold()])\n",
    "\n",
    "# Modify the weights and biases to create the logic gates\n",
    "# AND\n",
    "network.weights[0] = np.asarray([[1], [1]])\n",
    "network.biases[0] = np.asarray([-1.5])\n",
    "print('AND(A,B) = \\n', logic_gate(network))\n",
    "\n",
    "# OR\n",
    "network.weights[0] = np.asarray([[1], [1]])\n",
    "network.biases[0] = np.asarray([-0.5])\n",
    "print('OR(A,B) = \\n', logic_gate(network))\n",
    "\n",
    "# NAND\n",
    "network.weights[0] = np.asarray([[-2], [-2]])\n",
    "network.biases[0] = np.asarray([3])\n",
    "print('NAND(A,B) = \\n', logic_gate(network))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Training the network\n",
    "\n",
    "While interesting, the `Network` class above is pretty useless as it stands since there is no way to learn a function we don't already know.  In this step, we will add the ability to train our network on a dataset using the gradient descent and back-propagation algorithms.  Let's review both of these algorithms now.\n",
    "\n",
    "### (Stochastic) Gradient Descent\n",
    "\n",
    "Recall from the beginning of the lab that we want to train the network parameters by minimizing a given loss function over an entire dataset.  One method of doing this is using the gradient descent (GD) algorithm which you have already seen this in the lecture portion of the class, thus I will just provide the update rule here.\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\nabla_\\theta \\mathcal{L},\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate.  The update rule above is called GD because direction of change in the network parameters follow the opposite of the parameters' gradient in the loss function.  You can think of this like a ball rolling down a hill to find the minimum of the topology.  Only in this case, the ball is massless because it has no momentum.  When this update rule is applied to a random subset of the total dataset, it is called stochastic gradient descent (SGD).  SGD is far more efficient than GD when the batch size is large enough to approximate the true gradients while being significantly smaller than the full dataset.  Running over the entire dataset with SGD once is called an epoch of training.\n",
    "\n",
    "### Back-Propagation\n",
    "\n",
    "From the GD update rule above, it is clear we will need to compute the gradients of the network parameters with respect to the cost function.  This is exactly what back-propagation does, and thus is a crucial component to almost all neural network learning algorithms.  In the next exercise, we will derive the 4 equations in back-propagation.  You will need knowledge of the [chain rule for differentiation](https://en.wikipedia.org/wiki/Chain_rule) if you are not already familiar with this.\n",
    "\n",
    "### Exercise 5: Derive backprop formulas\n",
    "\n",
    "Before we start, it is convenient to define the following variable,\n",
    "\n",
    "$$\n",
    "\\delta^l \\equiv \\frac{\\partial \\ell_p}{\\partial z^l},\n",
    "$$\n",
    "\n",
    "Thus $\\delta^l$ is the gradient of the loss function for a point $p$ with respect to the input to the activation function for the layer $l$ in our network.\n",
    "\n",
    "1. What is the shape of $\\delta^L$? (same as output)\n",
    "2. Show that $\\delta^L = \\nabla_{a^L} \\ell_p  \\odot {\\sigma'}^L ( z^L )$. Hint: apply the chain rule to the definition of $\\delta^L$.\n",
    "3. Show that for $l < L$, $\\delta^l = [\\delta^{l+1} (W^{l+1})^T ] \\odot {\\sigma'}^l ( z^l )$.  Hint: try computing $\\delta^{L-1}$ and compare to $\\delta^L$.\n",
    "4. Show that $\\nabla_{W^l}\\ell_p = (a^{l-1})^T \\delta^l$. Hint: use definition of $z^l$.\n",
    "5. Show that $\\nabla_{b^l}\\ell_p = \\delta^l$.\n",
    "\n",
    "Note that these four formulas allow us to compute the gradient of the loss function for a single training point with respect to the parameters of our network.  Looking at the equations more closely, you should see a possible algorithm form.\n",
    "\n",
    "1. Compute the values of $z^l$ and $a^l$ for $l = {1, \\dots, L}$ by forward propagation through the network.  Recall $a^0 = x_p$.\n",
    "2. Use the equation in Exercise 5.2 above to compute $\\delta^L$.  \n",
    "3. Back-propagate in the reverse direction using the equation in Exercise 5.3 to get all the other $\\delta^l$ values.\n",
    "4. Compute the parameter gradients using the equations in Exercises 5.4 and 5.5.\n",
    "\n",
    "Note that this will yield the gradients for a single training point.  The gradients for the total loss function can easily be computed by\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathcal{L} = \\frac{1}{N} \\sum_{p=1}^N \\nabla_\\theta \\ell_p.\n",
    "$$\n",
    "\n",
    "Of course, the exact form of $\\nabla_\\theta \\ell_p$ will depend on the choice of $\\ell$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Implement the training algorithm\n",
    "\n",
    "Before we can implement the training algorithm, we need to define a cost function.  For now, let's create a class representing the quadratic cost which was given in the introduction.\n",
    "\n",
    "1. Show that the derivative of the quadratic cost is $\\nabla_{\\hat{y}} \\ell(\\hat{y}, y) = \\hat{y} - y$.\n",
    "2. Impement the quadratic cost function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticCost:\n",
    "\n",
    "    @staticmethod\n",
    "    def __call__(y_pred, y):\n",
    "        \"\"\"Computes the quadratic cost function.\"\"\"\n",
    "        return 0.5 * np.sum(np.square(y_pred - y))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(y_pred, y):\n",
    "        \"\"\"Computes the derivative of the quadratic cost function.\"\"\"\n",
    "        return (y_pred - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this cost defined, let's implement three remaining functions in `Network` that will allow us to train our models.  The first is the `backprop` method which takes a single training sample $x, y$ and a cost function object like the one above, and computes the gradient of the cost function using the backpropagation algorithm.\n",
    "\n",
    "3. Implement the `backprop` funtion in the `Network` class.\n",
    "\n",
    "Next, implement a single epoch of training by averaging the gradients over the whole dataset (calling `backprop` on each data point) and using the GD update rule to update the weights and biases.\n",
    "\n",
    "4. Implement the `update_step` function in the `Network` class.\n",
    "\n",
    "The `update_step` method is used by the `train` method successively in a loop as you can see in the `Network` class implementation above.  Assuming you have correctly implemented the `backprop` and `update_step` functions correctly, you should now be able to train the network on real training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Test training on simple regression problem.\n",
    "\n",
    "Run the following code to test the network.  If all is correct, you should see the totall loss dropping and the network will learn the sine function.  \n",
    "\n",
    "1. Play with different hyperparameters of the network and training such as the number of layers, number of nodes per layer, training rate, and number of training samples.  Can you improve on the training performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(-1, 1, (100, 1))\n",
    "y = np.sin(np.pi*x)\n",
    "\n",
    "network = Network([1, 100, 1], [Sigmoid(), Identity()])\n",
    "network.train((x, y), QuadraticCost(), 0.1, 10000)\n",
    "\n",
    "x_pred = np.arange(-1, 1, 0.01).reshape(200,1)\n",
    "y_pred = network.feed_forward(x_pred)\n",
    "\n",
    "plt.plot(x, y, 'k.')\n",
    "plt.plot(x_pred, y_pred, 'k-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Hand-written digit classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our model on a more sophisticated dataset, namely the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). This dataset includes 70,000 images of hand-written digits 28x28 pixels (=784), split into a training set of 60,000 images and a test set of 10,000 images. Each image is labeled with a digit from 0 to 9.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Download the MNIST dataset\n",
    "\n",
    "1. Just run the following cell to download the MNIST database and prepare it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/hsjeong5/MNIST-for-Numpy.git\n",
    "!mv ./MNIST-for-Numpy/mnist.py ./mnist.py\n",
    "import mnist\n",
    "mnist.init()\n",
    "x_train, y_train, x_test, y_test = mnist.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next check that the shapes of the training and test data are what you expext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get a feel for the dataset by viewing some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "plt.imshow(x_train[index,:].reshape(28, 28))\n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: Create a one-hot encoding of the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y_train` and `y_test` currently contain the digits 0-9, which is not suited for training as we want to perform a 10-class classification training on our network. To fix this, we need to perform a so-called one-hot encoding on the labels which convert each digit into a vector of length 10 full of zeros, except the element corresponding to the digit which should be 1.  For example, if our label is 3, then the corresponding one-hot encoding is [0, 0, 0, 1, 0, 0, 0, 0, 0, 0].\n",
    "\n",
    "1. Convert the `y_train` and `y_test` vectors into one-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10: Train a network to recognize digits\n",
    "\n",
    "1. Implement the cross-entropy loss function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "\n",
    "    @staticmethod\n",
    "    def __call__(y_pred, y):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(y_pred, y):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train a model with 1 hidden layers of 30 nodes each, using sigmoidal activations and a cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network for 30 epochs with a learning rate of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compute the accuracy of the model on the test data and compare a few predictions to their images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra\n",
    "\n",
    "At this point you have a fully functioning neural network model which can be used in real machine-learning problems.  There are many things which you could add at this point to make the model more effective.  Consider playing with some of the following:\n",
    "\n",
    "- Adding weight regularization\n",
    "- Using different activation functions such as ReLU or Tanh\n",
    "- Improve the efficiency by making the backprop algorithm work on whole batches at once\n",
    "- Implement the SGD algorithm\n",
    "- Redesign the model to allow plug-and-play layers to be stacked together, allowing more complicated layers such as convolutions to be used.\n",
    "\n",
    "Of course the list is endless, just have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step: Using Tensorflow\n",
    "\n",
    "As you have now seen, building a neural network and training from scratch can require a significant amount of effort.  Furthermore, the naive design of our `Network` class does not allow us to easily incorporate new kinds of layers such as convolutions, dropout, or batch normalization.  In practice of course, several libraries already exist that provide all of this functionality for your, so that you can focus on building a model for your specific dataset.  Here, we will show you how to redo the digit classification example above, with less than 10 lines of code using the Tensorflow library.  You can [find this example](https://www.tensorflow.org/tutorials) directly on the Tensorflow webiste if you are interested.  Let's walk through the code below.\n",
    "\n",
    "### Import Tensorflow and download the MNIST dataset\n",
    "\n",
    "Tensorflow includes a high-level neural network architecture building API called Keras which makes it easy to create neural network models using \"off-the-shelf\" layers.  In addition, Keras also provides commonly used datasets, such as the MNIST dataset.  In the first line below, Tensorflow is imported.  Second, the `keras.datasets` API is used to dowload the full dataset into two groups, a training set and a testing set, like you have seen before.  In the third line, the pixel data (integers between 0-255) is converted to real values between 0.0-1.0, making it easier for our NN model to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the NN model\n",
    "\n",
    "In the next two lines, the Keras API is used to create a simple NN model with the following layers:\n",
    "\n",
    "- `Flatten` takes an N-dimensional array and reshapes it into a 1D array. In this case, the images are 28x28 pixels, which get converted to a 784 1D tensor.\n",
    "- `Dense` is exactly like the type of implicit layers we coded in our `Network` class.  That is, they take some 1D input and apply a linear transformation before applying a nonlinear activation.  In this case we are using 512 nodes in the dense layer.\n",
    "- `Dropout` is a little more advanced.  As you have seen in the lecture, a dropout layer sets some percentage of the inputs to the layer to zero.  During training, each time data is propagated through the dropout layer, a different random set of nodes is zeroed out.  This forces the network to make more generalizations about the data and reduces overfitting.  When training is done, the dropout layer allows all the nodes to pass through, but multiplies the result by the dropout factor such that the sum remains the same.  In this case, 20% of the nodes are randomly cancled out during training and the result during evaluation is multiplied by 0.8.  \n",
    "- The final layer is a dense layer of 10 nodes corresponding to the one-hot encoding of the digit labels.  Note that a softmax activation function is used so that these nodes will represent propbabilities.\n",
    "\n",
    "In the last line, the model is \"compiled\" with an optimizer (Adam), loss function (sparse categorical crossentropy), and a metrics, which is responsible for providing useful output during training.  In this case, the `accuracy` metric is used which computes the total number of training samples that were correctly categorized as trianing progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model\n",
    "\n",
    "In the last step, we train the model.  This happens in the first line below, were we call the model's `fit` function, which takes the training data and a number of epochs.  The number of batches per epoch is handled by default.  In the second line, we evaluate the model on our testing data to see how well it does on data it has never seen before.  Note that we get a 98% accuracy with this simple model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
