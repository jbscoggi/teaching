{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 204 Lab 10: Dimensionality Reduction with PCA and Autoencoders\n",
    "\n",
    "J.B. Scoggins\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jbscoggi/teaching/blob/master/Polytechnique/CSE204/Lab10_answers.ipynb) \n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jbscoggi/teaching/master?filepath=Polytechnique%2FCSE204%2FLab10_answers.ipynb)\n",
    "\n",
    "\n",
    "## Dimension Reduction\n",
    "\n",
    "In this lab, you will get hands-on experience with dimension reduction using two separate techniques: Priciple Component Analysis and Undercomplete Autoencoders.  The goal of dimension reduction is to find a suitable transformation which converts a large dimensional space into a smaller feature space, such that the important information is not lost.\n",
    "\n",
    "## The MNIST Dataset\n",
    "\n",
    "We will use the MNIST digits dataset throughout this excercise.  The MNIST dataset provides 60000 28x28 pixel grayscale training images of hand-written digits 0-9.  The images are labeled with integer values 0-9.  The training set has become the defacto image classification example due to its small size.  For example, [follow the link](https://www.tensorflow.org/tutorials/) to see how to build a digit classifier with TensorFlow in less than 20 lines of code!\n",
    "\n",
    "In this excercise, we are not interested in classifying images of digits.  Instead, we will think of the images as defining a 28x28 = 784 element feature space.  In this context, we are interested in transforming the 784 parameters into a smaller set of transformed coordinates.  Before continuing to the next section, use the keras datasets module to load the MNIST dataset and get to know how it is structured.\n",
    "- Inspect the dataset. What is the shape of x_train and y_train?\n",
    "- Plot a few images using matplotlib.pyplot to see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.datasets.mnist as mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print( x_train.shape, x_test.shape)\n",
    "plt.imshow(x_train[0,:,:])\n",
    "plt.show()\n",
    "print( y_train.shape )\n",
    "print( y_train[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle Component Analysis (PCA)\n",
    "\n",
    "The goal of PCA is to perform an orthogonal transformation which converts a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables, called _principle components_.  This can be thought of as fitting an n-dimensional ellipsoid to the observations.  \n",
    "\n",
    "Let's consider a dataset $X\\in R^{n\\times p}$, where $n$ is the number of observations and $p$ the number of variables.  PCA transforms $X$ into a new coordinate system (new variable set), such that the greatest variance in the data is captured in the first coordinate, and then the second, and so on.  More specifically, the transformed coordinates $T \\in R^{n\\times p}$ are written as a linear combination of the original dataset,\n",
    "\n",
    "$$ T = X W, $$\n",
    "\n",
    "where $W \\in R^{p\\times p}$ is the transformation matrix.  The first column of $W$, denoted as $w_1$, is constructed to maximize the variance of the transformed coordinates.\n",
    "\n",
    "$$ w_1 = \\underset{\\|w\\|=1}{\\operatorname{argmax}} \\sum_{i=1}^{n} (t_1)_i^2 = \\underset{\\|w\\|=1}{\\operatorname{argmax}} \\| X w \\|_2^2 = \\underset{\\|w\\|=1}{\\operatorname{argmax}} \\frac{w^T X^T X w}{w^T w} $$\n",
    "\n",
    "The ratio in the last term is known as the _Rayleigh quotient_.  It is well known that for the positive, semidefinite matrix $X^T X$, the largest value of the Rayleigh quotient is given as the largest eigenvalue of the matrix, where $w$ is eigenvector associated with that eigenvalue.\n",
    "\n",
    "The remaining columns of $W$ can be found by finding the the next orthogonal linear combination which maximizes the variance of the data, minus the previously transformed coordinates.\n",
    "\n",
    "$$ w_k = \\underset{\\|w\\|=1}{\\operatorname{argmax}} \\| (X - \\sum_{s=1}^{k-1} X w_s w_s^T) w \\|^2_2 $$\n",
    "\n",
    "Practically, the columns of $W$ are typically computed as the eigenvectors of $X^T X$ ordered by their corresponding eigenvalues in descending order.\n",
    "\n",
    "### Singular Value Decomposition\n",
    "\n",
    "The Singular Value Decomposition of a matrix $X \\in R^{n\\times p}$ is given as\n",
    "\n",
    "$$ X = U \\Sigma W^T, $$\n",
    "\n",
    "where $\\Sigma \\in R^{n\\times p}$ is a rectangular diagonal matrix of positive values known as the the singular values, of $X$, $\\sigma(X)$, and $U \\in R^{n\\times n}$ and $W \\in R^{p\\times p}$ are orthonormal matrices, whose columns are the left and right (respectively) singular vectors of the matrix $X$.  Using this decomposition, we can easily see that\n",
    "\n",
    "$$ X^T X = W \\hat{\\Sigma} W^T, $$\n",
    "\n",
    "where $\\hat{\\Sigma}$ is a square diagonal matrix of the squared singular values of $X$.  Comparing this to the eigenvalue decomposition of $X^T X = Q \\Lambda Q^T$, we see that the singular values of $X$ represent the square-root of the eigenvalues of $X^T X$, and the singular vectors of $X$ are simply the eigenvectors of $X^T X$.  Therefore, we can compute perform PCA on a data matrix $X$ by computing its right singular vector matrix, $W$.\n",
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "We can reduce the dimensionality of our data by truncating the transformed variables to include only a subset of those variables with the highest variance.  For example, if we keep the first $L <= p$ variables, the reduced transformation reads\n",
    "\n",
    "$$ T_L = X W_L, $$\n",
    "\n",
    "where $W_L \\in R^{n\\times L}$ is the eigenvector matrix as before, but taking only the first $L$ columns.  This technique has been widely used to reduce the dimension of large-dimensioned datasets by accounting for the directions of largest variance in the data, while neglecting the other directions.  In addition, this can also be used to remove noise from a dataset, in which it is assumed that the noise accounts for a small degree of variance, compared to the true underlying parameterization.  Finally, using PCA to find the 2 highest varying parameters can also allow us to visualize a high-dimensional dataset.  Let's do this now on the MNIST images.\n",
    "\n",
    "- Use numpy to [compute the SVD](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html) of the MNIST images.  Note, you will first need to reshape the array to 2D with n = 60000 and p = 28 x 28 = 784.\n",
    "- Compute the first two principle components by truncating the eigenvector matrix before multiplying by the data array.\n",
    "- Plot the two principle components on a scatter plot with matplotlib.pyplot.scatter, using the image labels to color the markers.\n",
    "- What do you notice about the how the data is presented in the plot?\n",
    "- Which images form a tight cluster in the reduced space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Convert the training data into a M x N array where M is the number of samples and N is the number of parameters\n",
    "M = 2000\n",
    "data = x_train.reshape(60000, 784)[:M,:]\n",
    "labels = y_train[:M]\n",
    "print( data.shape )\n",
    "\n",
    "# Compute the covariance matrix\n",
    "u, s, vh = np.linalg.svd(data, full_matrices=False)\n",
    "print( u.shape )\n",
    "print( s.shape )\n",
    "print( vh.shape )\n",
    "\n",
    "T2 = np.matmul(data, vh.T[:,:2])\n",
    "print( T2.shape )\n",
    "\n",
    "plt.scatter(T2[:,0], T2[:,1], c=labels, cmap='rainbow')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scree Plot\n",
    "\n",
    "It is not always clear how many principle components are necessary to accurately represent the high-dimensional space.  There are two widely used methods to help us get a sense of the number variables required.  The first is called a Scree plot, which plots the eigenvalues of $X^T X$ in descending order.  Since the eigenvalues represent the degree of variance in the corresponding principle components, such a plot can tell use how many components are needed before we reach diminishing returns.\n",
    "- Plot the Scree plot for the MNIST data.\n",
    "- How many principle components are needed to represent most of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.square(s[:100]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Variance Explained\n",
    "\n",
    "Another method is called _Total Variance Explained_.  In this method, we plot the cumulative sum of the eigenvalues and choose the number of components which give us a certain percentage fo the total variance.\n",
    "- Plot the cumulative sum of the eigenvalues.\n",
    "- Plot a horizontal line at 95% of the total sum.\n",
    "- Based on this, how many components are needed to capture 95% of the variance?\n",
    "- How does this compare to the Scree plot result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum = np.cumsum(np.square(s))\n",
    "plt.plot(cumsum)\n",
    "plt.axhline(0.95 * cumsum[-1], c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct Images\n",
    "\n",
    "Now that we have an idea of how many principle components are necessary, let's use them to encode the images in a smaller set of features, which we can then decode to reconstruct the images from the lower-dimensional space.  Recall that based on the PCA transformation, we can compute the reconstructed images with\n",
    "\n",
    "$$ \\hat{X} = (X W_L) W_L^T $$\n",
    "\n",
    "- Create a grid of images using pyplot.subplots and imshow.\n",
    "  - In the first row, plot the first 5 images of the dataset.\n",
    "  - In the next 4 rows, plot reconstructions of the images using the first 5, 15, 30, and 100 principle component vectors.\n",
    "- How do the reconstructed images compare with the originals as you increase the size of the reduced space?\n",
    "\n",
    "Note that once we have computed the transformation matrix $W$, we essentially have a compression scheme to convert our images into a compressed format.  From this perspective, using the first 5, 10, 30, and 100 principle components is equivalent to compressing the data at a rate of 156:1, 78:1, 26:1, and 8:1, respectively.  By contrast, JPEG image compression can obtain compression ratios of 23:1 with reasonable image quality, surpassing the quality of reconstructions with PCA.  For that reason, PCA is not really used for image compression, but it has been used in a number of other fields, particularly in physics and engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some sample reconstructions next to originals\n",
    "fig, ax = plt.subplots(5,5)\n",
    "for i in range(5):\n",
    "    ax[0,i].imshow(data[i,:].reshape(28,28))\n",
    "    \n",
    "    for j, s in enumerate([5, 15, 30, 100]):\n",
    "        new_data = np.matmul(np.matmul(data, vh.T[:,:s]), vh[:s,:])\n",
    "        ax[j+1,i].imshow(new_data[i,:].reshape(28,28))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "\n",
    "In this next part, we will devise a compression strategy using another method, autoencoders.  Autoencoders are neural networks which are trained to output their input in such a way that they learn a reduced dimensional space of the input distribution.  They are general composed of two distinct layers. The first encodes the input space (encoder) and the second decodes the encoded space back to the original feature space (decoder).  There are 3 basic types of autoencoders:\n",
    "1. __Undercomplete__ autoencoders work by constructing a network that has a hidden code layer that has fewer nodes than the input and output layers.  After training, the smaller hidden layer will represent an encoding of the input onto a lower dimensional space.\n",
    "2. __Regularized__ autoencoders use various regularization terms in the loss function during training to constrict the space of the output.  For example, sparse autoencoders add a sparsity regularization term in the loss to force as many nodes in the hidden layers to be zero.\n",
    "3. __Variational__ autoencoders work slightly differently than the previous two.  In this case, the autoencoder learns parameters that model the distribution of the input data in the encoder.  The decoder is then used to reconstruct the output based on a random sample from this distribution.  Some variational autoencoders have been used for image generation. \n",
    "\n",
    "In this exercise, we will construct two undercomplete autoencoders and train them on the MNIST data as before.\n",
    "\n",
    "### Dense linear decoder\n",
    "\n",
    "It is well known that an autoencoder with a linear decoder layer and a mean-squared-error loss function will learn the same feature space as PCA.  Let's check this by creating a simple linear autoencoder.\n",
    "\n",
    "- Write a funtion which takes the `input_size` and the `reduced_size` and returns an autoencoder model using Keras.\n",
    "  - The autoencoder should be comprised of\n",
    "    - A dense encoder layer taking `input_size` inputs with `reduced_size` nodes and ReLU activation.\n",
    "    - A linear decoder layer with `input_size` nodes.\n",
    "  - Compile the model using the Adam optimizer and MSE loss\n",
    "  - In addition to the autoencoder, return another model which just takes the input and returns the output of the encoder layer.  For hints, see the [functional API documentation](https://keras.io/getting-started/functional-api-guide/).\n",
    "- Using your function, create a linear autoencoder with `input_size` = 784 and `reduced_size` = 2.\n",
    "- Train the model using the MNIST data as input and output for 20 epochs.\n",
    "- Plot the history of the loss versus the epoch number to make sure training is basically complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "\n",
    "def linear_autoencoder(input_size, code_size):\n",
    "    # Build the autoencoder\n",
    "    input = Input(shape=(input_size,))\n",
    "    code = Dense(code_size, activation='relu')(input)\n",
    "    output = Dense(input_size)(code)\n",
    "\n",
    "    # Compile the model\n",
    "    autoencoder = Model(input, output)\n",
    "    autoencoder.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "    \n",
    "    # Also provide just the encoder model\n",
    "    encoder = Model(input, code)\n",
    "    return autoencoder, encoder\n",
    "\n",
    "linear_ae, linear_encoder = linear_autoencoder(784, 2)\n",
    "history = linear_ae.fit(data, data, epochs=20)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the trained encoder to encode the MNIST data to 2 variables.\n",
    "- Plot the two components in a scatter plot as with the PCA result.\n",
    "- How does the scatter plot compare to the one you made with PCA?  Recall that this autoencoder should learn the same vector space as PCA, though it will not learn the exact same transformation (could be rotated, scaled, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = linear_encoder.predict(data)\n",
    "plt.scatter(T[:,0], T[:,1], c=labels, cmap='rainbow')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear Decoder\n",
    "\n",
    "We saw in the previous section that linear decoders and MSE loss produce the same result as PCA.  Therefore, we can see nonlinear decoders as a nonlinear generalization of PCA.  By allowing nonlinear transformations, we should be able to increase the expressiveness of our reduced variables.  \n",
    "- Copy your linear autoencoder function, naming it differently to produce a nonlinear AE.\n",
    "- Add a dense hidden layer in between the encoder output and decoder output layers.  Give the hidden layer `input_size`/2 nodes and use ReLU activation.\n",
    "- Create the nonlinear AE using 2 variables as with the linear model.\n",
    "- Train as before and plot the loss history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_autoencoder(input_size, code_size):\n",
    "    # Build the autoencoder\n",
    "    input = Input(shape=(input_size,))\n",
    "    code = Dense(code_size, activation='relu')(input)\n",
    "    hidden = Dense(input_size/2, activation='relu')(code)\n",
    "    output = Dense(input_size)(hidden)\n",
    "\n",
    "    # Compile the model\n",
    "    autoencoder = Model(input, output)\n",
    "    autoencoder.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "    \n",
    "    # Also provide just the encoder model\n",
    "    encoder = Model(input, code)\n",
    "    return autoencoder, encoder\n",
    "\n",
    "nonlinear_ae, nonlinear_encoder = nonlinear_autoencoder(784, 2)\n",
    "history = nonlinear_ae.fit(data, data, epochs=20)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot the scatter plot of the reduced variables.\n",
    "- What can you say about grouping of points using the nonlinear model?  Does it seem to cluster the digits better than with the linear one? (If it doesn't like any better, try training the model again.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = nonlinear_encoder.predict(data)\n",
    "plt.scatter(T[:,0], T[:,1], c=labels, cmap='rainbow')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct Images\n",
    "\n",
    "Finally, let's use our autoencoders to produce reconstructed images from the MNIST data as we did with PCA.\n",
    "- Train linear and nonlinear autoencoders on the MNIST data using a `reduced_size` of 15.\n",
    "- Compare the loss histories of the training for both models on the same plot.  What does this tell you about the expressiveness of the two models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train autoencoder models\n",
    "lin_ae, _ = linear_autoencoder(784, 15)\n",
    "lin_hist = lin_ae.fit(data, data, epochs=20)\n",
    "nln_ae, _ = nonlinear_autoencoder(784, 15)\n",
    "nln_hist = nln_ae.fit(data, data, epochs=20)\n",
    "\n",
    "plt.plot(lin_hist.history['loss'])\n",
    "plt.plot(nln_hist.history['loss'])\n",
    "plt.legend(['linear', 'nonlinear'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the two AEs to produce reconstructed images.\n",
    "- Generate a grid of images\n",
    "  - The first row should contain the first 5 images in the MNIST set as before.\n",
    "  - The second row should contain their reconstruction using the linear model.\n",
    "  - The third row shoudl contain the reconstructions using the nonlinear model.\n",
    "- How well do each of the models reproduce the images? \n",
    "- How do they compare to the PCA reconstructions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some sample reconstructions next to originals\n",
    "fig, ax = plt.subplots(3,5)\n",
    "for i in range(5):\n",
    "    ax[0,i].imshow(data[i,:].reshape(28,28))\n",
    "    \n",
    "    new_data = lin_ae.predict(data)\n",
    "    ax[1,i].imshow(new_data[i,:].reshape(28,28))\n",
    "    \n",
    "    new_data = nln_ae.predict(data)\n",
    "    ax[2,i].imshow(new_data[i,:].reshape(28,28))\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
