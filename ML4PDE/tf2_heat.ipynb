{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf2_heat",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egtxcOIX9sCw",
        "colab_type": "text"
      },
      "source": [
        "# 1D Heat Equation with Tensorflow 2.x\n",
        "\n",
        "In this tutorial, you will learn how to solve the one-dimensional heat equation using the Tensorflow library.  Specifically, let's consider the following problem:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "&\\frac{\\partial u}{\\partial t} = \\alpha \\frac{\\partial^2 u}{\\partial x^2}, \\quad u = u(t,x), \\quad x\\in[0,1],\\\\\n",
        "&u(0,x) = \\sin(2\\pi x),\\\\\n",
        "&u(0,t) = u(1,t) = 0,\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where $\\alpha > 0$ is the thermal diffusivity of the material.  It is easy to verify that the exact solution to the above equation is\n",
        "\n",
        "$$\n",
        "u(t,x) = \\exp(-\\alpha\\, 4\\,\\pi^2\\, t) \\sin(2\\pi x).\n",
        "$$\n",
        "\n",
        "We can let $\\alpha = 1/4\\pi^2$ to simplify the problem definition.\n",
        "\n",
        "*Run the next 2 cells to setup Colab with TF2 and import the necessary packages.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsYvFY8E2dHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tell Colab we want to use TF2\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tcc0EmK2mRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import desired packages\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "K.backend.set_floatx('float64')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpKmoCgkD-pW",
        "colab_type": "text"
      },
      "source": [
        "## Setup the neural network model\n",
        "\n",
        "Next let's create a neural network which will be part of the solution representation.  This is easily done with the Keras API in tensorflow as shown in the next cell.  The network is a simple feed-forward, dense network with hyperbolic tangent activations and a linear output layer of size 1.  Note that the input dimension is not specified, Keras will automatically discover the input dimension during the first pass of data through the network.\n",
        "\n",
        "*Run the next cell to setup the network.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYo4wqwr2qmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the NN model\n",
        "network = K.Sequential()\n",
        "network.add(K.layers.Dense(16, 'tanh'))\n",
        "network.add(K.layers.Dense(16, 'tanh'))\n",
        "network.add(K.layers.Dense(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ERrIm-QE8gi",
        "colab_type": "text"
      },
      "source": [
        "## Define the solution and problem setup\n",
        "\n",
        "With our neural network model ready, we can now write our solution as a function of the neural network and implement the heat equation.  Since we have a simple problem in 1D, the boundary and initial conditions are most easily implemented by construction.  Therefore, we will write the solution as\n",
        "\n",
        "$$\n",
        "\\hat{u}(t,x) = g(x) + t\\,x\\,(x-1)\\;N(2t-1,2x-1),\n",
        "$$\n",
        "\n",
        "where $g(x) = \\sin(2\\pi x)$ is the initial condition and $N$ is the neural network which will take a time and space coordinate.  Note that we have scaled the coordinates to be between -1 and 1 when passing them to the network function in order to leverage the full range of the first hyperbolic tangent activation.\n",
        "\n",
        "### Computing gradients\n",
        "\n",
        "We will make use of the gradient tape feature of Tensorflow 2.x, which allows us to watch certain variables during a calculation and access gradients with respect to those variables afterword.  This same method of computing gradients will be used later to train the network from the gradient of the loss function with respect to the network parameters.  See [Automatic differentiation and gradient tape](https://www.tensorflow.org/tutorials/customization/autodiff) from the TF documentation for more information.\n",
        "\n",
        "*Run the next cell which implements the solution and heat equation residual.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STxBRkuD3U86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initial_condition(x):\n",
        "  return tf.sin(2*np.pi*x)\n",
        "\n",
        "def solution_1d(t, x):\n",
        "  return t*x*(1.0-x)*network(tf.concat([2*t-1,2*x-1], 1)) + initial_condition(x)\n",
        "\n",
        "def heat_equation_1d(t, x):\n",
        "  alpha = 1.0/(4.0*np.pi*np.pi)\n",
        "  with tf.GradientTape() as tape1:\n",
        "    tape1.watch(x)\n",
        "    with tf.GradientTape() as tape2:\n",
        "      tape2.watch([t,x])\n",
        "      u = solution_1d(t, x)\n",
        "    u_t, u_x = tape2.gradient(u, [t, x])\n",
        "  u_xx = tape1.gradient(u_x, x)\n",
        "  return u_t - alpha*u_xx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp7hxaWSLiRi",
        "colab_type": "text"
      },
      "source": [
        "## Plot the solution\n",
        "\n",
        "Now that we have the solution defined, we can go ahead and plot it to see what it looks like, before we train the network.  Note that the network parameters are intialized to random values (see docs for [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)), meaning the solution will look different for everyone.  However, based on how we have constructed the solution from the neural network, the boundary and initial conditions should be satisfied.\n",
        "\n",
        "*Run the next cell to plot the initial network solution, compared to the exact solution.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Phr1pZ5cvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the solution at half-second intervals\n",
        "def plot_solution():\n",
        "    x = np.reshape(np.linspace(0,1,100), (100,1))\n",
        "    for i,c in enumerate(['k', 'b', 'g', 'r']):\n",
        "        t = 0.5 * i * np.ones((100,1))\n",
        "        u_network = solution_1d(t, x).numpy()\n",
        "        u_exact = np.exp(-t)*np.sin(2*np.pi*x)\n",
        "        plt.plot(x, u_network, c+'-')\n",
        "        plt.plot(x, u_exact, c+'--')\n",
        "    plt.legend(['Network', 'Exact'])\n",
        "\n",
        "plot_solution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6YL_Yc4MWtV",
        "colab_type": "text"
      },
      "source": [
        "## Define the loss function\n",
        "\n",
        "Next let's define the loss function.  Recall that this will be the average residual squared error, evaluated on our set of collocation points, which will be passed into the loss function.\n",
        "\n",
        "*Run the next cell to define the loss function.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNg810LO-GFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(t, x):\n",
        "  pde_loss = tf.reduce_mean(\n",
        "    tf.square(heat_equation_1d(t, x)))\n",
        "  return pde_loss "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W3t_tXjMrPV",
        "colab_type": "text"
      },
      "source": [
        "## Train the network solution\n",
        "\n",
        "With the network, solutio, PDE, and loss function defined, it is time to actually train the network on our PDE.  Training is done in the following steps:\n",
        "\n",
        "1. Define an optimizer: this selects the optimization algorithm to use.  We will use the Adam algorithm which is a gradient descent method incorporating momentum and adaptive learning rate scaling.\n",
        "2. Define our collocation points: here we will simply use a grid of 50x50 points on the $(t,x)$ domain.\n",
        "3. For each epoch:\n",
        "  1. Compute the current loss and it's gradient w.r.t. network parameters\n",
        "  2. Apply the gradients with the optimizer to update the network parameters\n",
        "\n",
        "That's it!\n",
        "\n",
        "*Run the next cell train the network for 500 epochs.*\n",
        "\n",
        "*After it's done, replot the solution above and see how it compares to the exact solution.*\n",
        "\n",
        "Note that you can always run the following cell again to train for another 500 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvA0X1iND2GS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grad(t, x):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_value = loss(t, x)\n",
        "  return loss_value, tape.gradient(loss_value, network.trainable_variables)\n",
        "\n",
        "def train(num_epochs=500):\n",
        "  optimizer = K.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "  t, x = np.meshgrid(np.linspace(0,2,50), np.linspace(0,1,50))\n",
        "  t = tf.convert_to_tensor(np.reshape(t, (-1,1)))\n",
        "  x = tf.convert_to_tensor(np.reshape(x, (-1,1)))\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    loss_value, grads = grad(t, x)\n",
        "    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
        "    if epoch % 100 == 0:\n",
        "      print(f'{epoch}: loss = {loss_value}')\n",
        "\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh0BcAsV7esn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}